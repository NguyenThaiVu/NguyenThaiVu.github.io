---
layout: post
title:  "Milestone 2"
date:   2023-11-14 15:25:05 -0400
categories: Data science project
---

# Table of content:
- [1. Feature Engineering 1](#1-feature-engineering-1)
- [2. Baseline Model](#2-baseline-model)
    - [2.1. Unveiling the Accuracy Paradox](#21-unveiling-the-accuracy-paradox)
    - [2.2. Evaluating Baselines](#22-evaluating-baselines)
- [3. Feature Engineering 2](#3-feature-engineering-2)
    - [3.1. Add Features](#31-add-features)
    - [3.2. Add Json File](#32-add-json-file)
- [4. Advanced Model](#4-advanced-model)
    - [4.1. Baseline Xgb](#41-baseline-xgb)
    - [4.2. Xgb Hyperparameter Tuning](#42-xgb-hyperparameter-tuning)
    - [4.3. Xgb Feature Selection](#43-xgb-feature-selection)
- [5. Best Shot Model](#5-best-shot-model)
    - [5.1. Data Pre-processing](#51-data-pre-processing)
        - [5.1.1. Add Feature](#511-add-feature)
        - [5.1.2. Feature Selection - Correlation](#512-feature-selection---correlation)
        - [5.1.3. Feature Selection - Mutual Information](#513-feature-selection---mutual-information)
        - [5.1.4. Balance Dataset](#514-balance-dataset)
    - [5.2. Machine Learning Model Grid Search](#52-machine-learning-model-grid-search)
    - [5.3. Log the Models to the Experiments on Comet.ml](#53-log-the-models-to-the-experiments-on-cometml)
- [6. Evaluation](#6-evaluation)
    - [6.1. Evaluate on Regular Season](#61-evaluate-on-regular-season)
    - [6.2. Evaluate on Playoff Game](#62-evaluate-on-playoff-game)

---

## 1. Feature Engineering 1

This figure below shows how the shot counts vary by distance from the net. As expected, the number of shots and goals decreases as the distance increases, since players tend to shoot closer to the net and scoring from far away is harder. In fact, there are no goals beyond 75 feet. Interestingly, the bin closest to the net does not have the highest number of shots or goals, which may be because it is more difficult to shoot past the goalie at a short range.

![alt text](/Images/feature_engineering1/distance.png)

This figure below shows how the shot counts and goals vary by the angle from the net. We can see that the probability of scoring a goal is higher when the angle is closer to zero, which means that the shooter is facing the net directly. However, the majority of the shots are taken from angles between -0.5 and 0.5 radians, which means that the shooters are slightly off-center. The goals do not follow this trend, and the highest number of goals is achieved at around 0 radian, which means that the shooters are more accurate when they are aligned with the net.

![alt text](/Images/feature_engineering1/angle.png)

This figure below shows how the angle from the net changes with the distance from the net for the shots and goals. We can see that the range of angles decreases as the distance increases, which means that the players shoot with smaller angles when they are farther from the net. This may be because shooting with larger angles from far away is less likely to score or hit the target. Moreover, the figure is symmetric, which means that the sign of the angle does not matter, and the players shoot equally from the left and right sides of the rink. However, the distribution of angles is different for the shots and goals near the net. The shots have a wider range of angles, and some of them are very large, which means that the players shoot from sharp angles close to the net. 

![alt text](/Images/feature_engineering1/angle_distance.png)

This figure below shows how the probability of scoring a goal varies by the angle from the net. We can see that the probability is symmetric with respect to the sign of the angle, which means that it does not matter whether the shot is taken from the left or right side of the rink. However, the probability is higher when the angle is closer to zero, which means that shooting near the center of the rink or aligning with the net increases the chance of scoring a goal. The highest probability is achieved at around 0 radian, which corresponds to facing the net directly.

![alt text](/Images/feature_engineering1/goal_rate_angle.png)

This figure below shows how the probability of scoring a goal changes with the distance from the net. We can see that the probability drops sharply at first, and then decreases gradually as the distance increases. This makes sense because it is harder to score from far away. However, the probability is not zero beyond a certain distance, and it fluctuates slightly. This may be because there are some outliers or exceptional shots that score from a long distance.

![alt text](/Images/feature_engineering1/goal_rate_distance.png)

This figure below shows the effect of distance from the net and net emptiness on the probability of scoring a goal. As explained in the document, the probability of scoring a goal with a non-empty net decreases as the distance increases, which is expected because it is harder to shoot past the goalie from far away. However, the probability of scoring a goal with an empty net does not follow the same trend. Although the probability is very low even near the net, because it is rare that the net is empty, there are some cases where the players score from a long distance when the net is empty. This may be because the players take advantage of the situation and try to shoot from their own zone or the neutral zone.

![alt text](/Images/feature_engineering1/empty_net.png)



---
## 2. Baseline Model

In the provided notebook, we implemented baseline models for predicting goal outcomes in hockey games. The notebook begins by importing necessary libraries, setting up directories for data, models, and figures, and initializing Comet experiments for experiment tracking. It then loads the dataset, preprocesses it by selecting desired features, and splits it into training and validation sets. Logistic regression models are trained separately for shot distance, shot angle, and a combination of both features. The notebook includes visualizations such as confusion matrices, ROC curves, goal rate, cumulative percentage of goal, and calibration curves for each model. Random predictions are also generated as a baseline. The notebook concludes with logging relevant metrics and saving the trained models, confusion matrices, and metrics on Comet for further analysis. The models and experiments are tagged appropriately, indicating their baseline nature and the features used in training.

### 2.1. Unveiling the Accuracy Paradox

The histogram of the labels in the training dataset:
![](/Images/baseline/hist.png)

The evaluation results indicate a high accuracy of 91% on the validation set, but upon closer inspection of the precision, recall, and f1-score for label 1 (goal), it becomes apparent that the model is unable to correctly predict instances of this class, yielding zeros in these metrics. 

The issue arises from the significant class imbalance in the dataset, where label 0 (non-goal) vastly outnumbers label 1. With 276,782 samples for label 0 and only 29,032 samples for label 1, the model might be biased towards predicting the majority class, achieving high accuracy due to the dominance of label 0 in the dataset. However, this high accuracy is misleading, as the model struggles to capture the minority class (label 1).


### 2.2. Evaluating Baselines

![](/Images/baseline/baseline-evaluation.png)

As we can see, the simplest baseline, random prediction, acts exactly the same as random models, having 50 percent area under the curve in the ROC plot and steady in goal rate and calibration plots. 

On the other hand, we have noticed slight improvement from the angle model to the distance model and then from there to the model using a combination of them showing that as we increase the number of features, we encounter better performance in evaluation.


We add the experiment to the comet. You can access to those experiment by the following links:

- Baseline Model - Distance: [here](https://www.comet.com/ift6758-b09-project/ift6758-project-milestone2/fd6f683bf9324bc4aafe732516e9ed38)

- Baseline Model - Angle: [here](https://www.comet.com/ift6758-b09-project/ift6758-project-milestone2/066eb71923294143887d23136514beb5)

- Baseline Model - Distance + Angle: [here](https://www.comet.com/ift6758-b09-project/ift6758-project-milestone2/90775941400f48689503c7bacdc0ff09)



---
## 3. Feature Engineering 2

### 3.1. Add Features

We added the features from the list below

| Feature Name                | Explanation                                                              |
|-----------------------------|--------------------------------------------------------------------------|
| game_second                 | Time (in seconds) of each event relative to the start of the entire game |
| last_event_type             | Type of the last event.                                                  |
| coor_x_last_event           | X-coordinate of the last event.                                          |
| coor_y_last_event           | Y-coordinate of the last event.                                          |
| time_last_event             | Time elapsed since the last event (seconds).                             |
| distance_last_event         | Distance from the last event.                                            |
| is_rebound                  | True if the last event was a rebound, otherwise False.                   |
| Change in shot angle        | Angle change in the shot, only applicable for rebounds.                  |
| Speed                       | Defined as distance from the previous event divided by time.             |
| power_play_time **(bonus)** | Time elapsed since the start of the power play (seconds).                |
| n_friend        **(bonus)** | Number of friendly non-goalie skaters on the ice.                        |
| n_oppose        **(bonus)** | Number of opposing non-goalie skaters on the ice.                        |


After adding the features, we processed the data:
- First we split the data into a training set and a test set (the function `split_train_test()` was used here), and the subsequent processing of the data will be different depending on the model chosen. 
- Then we removed features that were not relevant to the modeling (the function `remove_extra_features()` was used) and added a new feature `game_second`. 
- Finally we uploaded `train.csv`, `test_regular.csv`, `test_playoff.csv` to comet artifact.
You can download the dataset by the link [here](https://www.comet.com/ift6758-b09-project/artifacts/dataset/4.0.2)

### 3.2. Add Json File

- For this particular match, we wrote a separate py file `get_game.py` to run it, and the purpose of this file is to upload this match as a dataframe to comet.ml, named according to requirements(`wpg_v_wsh_2017021065.csv`)
- You can check the link: [here](https://www.comet.com/ift6758-b09-project/ift6758-project-milestone2/c2985f085ae342bcbd8aa86aa9b3cb3c?experiment-tab=panels&showOutliers=true&smoothing=0&xAxis=step)


---
## 4. Advanced Model
### 4.1. Baseline Xgb

To evaluate the model performance more reliably and robustly, I used repeated k-fold cross-validation with k=5 and 10 repetitions. This technique splits the data into five folds and trains and validates the model on different combinations of them. Then, it averages the results over the repetitions to reduce the variance of the estimate [^1]. The following figures show the results of this section.

[This is the link to the experiment.](https://www.comet.com/ift6758-b09-project/ift6758-project-milestone2/bd6de33ca21d428da3dfbad5d8a45538)

Overall, the performance (accuracy and ROC score) between the logistic regression and baseline xgb are the same. The table below compare the accuracy and ROC score between logistic regression and xgb:

|                                 | Accuracy |  ROC      |
| --------------                  | -------- | -------- |
| Logistic regression (dist+ang)  |  90.7%  |  0.710    |
| XGB                             |  90.7%   | 0.714 | 


[^1]: For this section i have used the codes of this [website](https://towardsdatascience.com/pooled-roc-with-xgboost-and-plotly-553a8169680c#:~:text=To%20get%20a%20ROC%20curve,validation%20and%20got%20500%20results.)


You can access the xgboost baseline experiment [here](https://www.comet.com/ift6758-b09-project/ift6758-project-milestone2/345cc3e407964868ad89ed84d7427932)

### 4.2. Xgb Hyperparameter Tuning
We selected 'max_depth', 'n_estimators', 'learning_rate', and 'reg_alpha' as the hyperparameters that have the most influence on the model accuracy. We used the scikit-learn GridSearchCV function to find the best values for these hyperparameters. The best settings for the hyperparameters are: 

```
'objective': ['binary:logistic'],
'max_depth': [3, 5, 7],
'learning_rate': [0.1, 0.01],
'reg_alpha': [0.1, 0.001]
```

<!-- ```
'eval_metric': 'logloss', 'learning_rate': 0.1, 'max_depth': 7, 'objective': 'binary:logisitc', and 'reg_alpha': 0.1. 
``` -->

The following figure show the best results of hyper-parameter tuning.
![](/Images/xgb/hp.png)


We put the compared image at the end of section 5. As you can see from the figures, the AUC score increases noticeably by almost 0.06. This means that the model can better distinguish between positive and negative classes. In addition, by looking at the calibration curve, we can say that now the model is better calibrated and performs well enough on the validation set. A well-calibrated model is one that produces probabilities that match the true outcomes. For example, if the model predicts a 70% chance of rain, it should rain 70% of the time.

We have add the experiment for this step, you can access it [here](https://www.comet.com/ift6758-b09-project/ift6758-project-milestone2/26fd18671492465ab4796b5272a54255).

You can also download the xgboost model with hyperparameter tuning from the model registry by this link [here](https://www.comet.com/api/registry/model/item/download?modelItemId=LdIkYK3Xjpz4CByoXrJnfAnjd)


### 4.3. Xgb Feature Selection

First of all, we created a correlation matrix for the features and set a threshold of 90% in order to remove the redundant features. After applying this, we found that period and game_second were correlated and we deleted one of them randomly. After that, we performed the xgboost plot_importance function and we got the result like the figure (a). As you can see, by setting our threshold to 200, we can keep the important features and delete the rest. After this, we also calculated SHAP values for the data, which are visible in the figure (b). SHAP values measure how much each feature contributes to the model prediction, either positively or negatively. After doing these two methods, we kept the features that both of them had found important, which are the following. 

```
['shot_distance', 'time_last_event', 'angle',
'y-coordinate', 'Change in shot angle',
'distance_last_event', 'game_second',
'Speed', 'coor_y_last_event', 'x-coordinate',
'power_play_time']
```

The feature importance of the trained xgb (figure a) and the shap value (figure b):
![](/Images/xgb/combine_feature_importance.png)

<!-- ![alt text](/Images/xgb/feature_importance.png)
![alt text](/Images/xgb/shap_importance.png) -->


As you can see, by choosing the features that we obtained from the feature selection techniques, the ROC curve is lower than before doing this, but this model is more calibrated, which means it has more confidence about its predictions. A lower ROC curve means that the model has a lower accuracy, or the ability to correctly classify positive and negative cases. However, a more calibrated model means that the model's predicted probabilities are closer to the true probabilities, or the actual outcomes.

We have add the experiment for this step, you can access it [here](https://www.comet.com/ift6758-b09-project/ift6758-project-milestone2/e8954f79d8c54d9cad608bae02b631a1)

You can also download the xgboost with feature selection from the model registry [here](https://www.comet.com/api/registry/model/item/download?modelItemId=4qHFYytx3ddoDtMfDAfWgJD2A)

Four figures (ROC/AUC curve, goal rate vs probability percentile, cumulative proportion of goals vs probability percentile, and the reliability curve) to three differents xgb models: baseline xgb, hyperparameter xgb, and feature selection xgb:

![](/Images/xgb/xgb_summary.png)


---
## 5. Best Shot Model

### 5.1. Data Pre-processing

In this section, we will provide step-by-step our data pre-processing step, including: add feture, feature selection using correlation, feature selection using mutual information, and balance dataset.

#### 5.1.1. Add Feature

In this section, we will add the new feature called `attacking_zone_shot`. This feature indicates whether a shot took place within the opposing team's attacking zone. We choose to analyze this feature based on the observation that the shot in the attacking zone has high chance to become a goal.

After analyzing, we've discerned that 94.5% of shots occurred within the attacking zone, while the remaining 5% happened outside this zone.

#### 5.1.2. Feature Selection - Correlation

In this section, we will check the correlation value between pairs of features. If the correlation between two features is larger than the specific `threshold`, we will remove it. In this experiment, we set the threshold is 0.9.

After analyzing, the correlation between `period` and `game_second` is 0.94. Therefore, we remove features `['period']`.

#### 5.1.3. Feature Selection - Mutual Information

On this section, we will use mutual information to identify the relationship between features and the target variable. The intuition behind this experiment is that higher mutual information indicates stronger predictive power.

After calculating the mutual information between each feature and target variable. We got the result
![](/Images/best_shot_model/mutual_information.jpg)

According to the experiment, we will set the threshold below **0.01**. In other words, If the mutual information between each feature and target variable is smaller than 0.01, we will remove it. 

After analyzing, we choose feature:
`['x-coordinate', 'y-coordinate', 'shot_distance', 'angle', 'isEmptyNet', 'n_friend', 'n_oppose', 'last_event_type', 'is_rebound', 'attacking_zone_shot']`

#### 5.1.4. Balance Dataset

Because the distribution of label in the training set is imbalanced. We apply the over-sample to add more samples to the minority class.

### 5.2. Machine Learning Model Grid Search

In this section, we run two type of models: decision tree and logistic regression with the preprocessed dataset. For the purpose of this experiment, we just run the simple grid search experiment and leave the complicated work later. The grid hyperparameter tuning on each model:
- Grid search decision tree:
```
param_grid_tree = {
    "tree__criterion": ['gini', 'entropy'],
    "tree__max_depth": [5, 10],
    "tree__min_samples_leaf": [5, 10]
}
```

- Grid search logistic regression:
```
param_grid_linear = {
    "linear_clf__penalty": ['l1', 'l2'],
    "linear_clf__C": [0.1, 0.01]
}
```
We create four figures (ROC/AUC curve, goal rate vs probability percentile, cumulative proportion of goals vs probability percentile, and the reliability curve) to both ML model: decision tree and logistic regression.

![](/Images/best_shot_model/best_shot_model.png)

<!-- #### i. The ROC/AUC curve
![](/Images/best_shot_model/roc.jpg)

#### ii. The goal rate vs probability percentile
![](/Images/best_shot_model/goal_rate.jpg)

#### iii. The cumulative proportion of goals vs probability percentile
![](/Images/best_shot_model/goal_cumulative_proportion.jpg)

#### iii. The reliability curve
![](/Images/best_shot_model/calibration.jpg) -->


In addition, we also inspect the accuracy, f1 score, and the  ROC score on the validation set:

|                | Accuracy | F1 score | ROC      |
| -------------- | -------- |--------  | -------- |
| Decision Tree  |  90.7%  | 0.738     | 0.74     |
| Logistic regression  |  90.3%  | 0.731     |  0.73 | 

In summary, the decision tree is the best model we can build.

### 5.3. Log the Models to the Experiments on Comet.ml

We have include multiples experiment on the comet. In case you want to check the metrics of each experiment in a tabular format, you can check via this link: [here](https://www.comet.com/ift6758-b09-project/ift6758-project-milestone2/view/new/experiments)


We have add the trained decision tree into our Model Registry. You can download it with this link:
- Decision tree model: [here](https://www.comet.com/api/registry/model/item/download?modelItemId=ezJQ3TR5dmuvrVucdWsbKtYaR)

We also add the evaluation metrics to the Comet experiment. We add 3 metrics, including: Accuracy, ROC, and confusion matrix. 
You can access each individual experiment with the tag `best_shot_model` for more details. 


---
## 6. Evaluation

Overall, our model perform well on both regular and playoff game, which mean that the generalization ability of our model is good.

### 6.1. Evaluate on Regular Season

On the logistic regression, the features combining distance and angle is better than each feature seperately. 

The performance of the decision tree model is better than the baseline logistic regressions. One reason to explain that we apply more features and we balance the dataset. 

Finally, the XGBoost model gets the highest ROC scores compared to other methods. One reason to explain for this scenarion is that the XGBoost is quite suitable for this type of tabular data. Moreover, our team perform the good feature selection process (part 5).

Our models perform with the approximately same ROC, goal rate, cumulative proportion, during the validation and testing set.

The ROC score of different models on the regular season compared to the validation set:

|        Model                     | Valivation set  | Test regular season | 
| --------------                   |        ------   | --------  |
| Logistic regression (dist)       | 0.69            |   0.69  |
| Logistic regression (angle)      | 0.57            |   0.56  |
| Logistic regression (dist+angle) | 0.71            |   0.71  |
| XGBoost                          | 0.77            |   0.78  |
| Decision Tree                    | 0.74            |   0.75  |



The plot curve of ROC/AUC, Goal rate vs probability percentile, Cumulative proportion of goals vs probability percentile, and Reliability curve of the regular games.
![](/Images/evaluation/regular_evaluation.png)


### 6.2. Evaluate on Playoff Game

The ROC score of different models on the playoff game compared to the validation set:

|        Model                     |Valivation set   | Test playoff games  |
| --------------                   |        ------   | --------  |
| Logistic regression (dist)       | 0.69            |   0.68  |
| Logistic regression (angle)      | 0.57            |   0.57  |
| Logistic regression (dist+angle) | 0.71            |   0.70  |
| XGBoost                          | 0.77            |   0.70  |
| Decision Tree                    | 0.74            |   0.75  |

There is slightly difference between ROC on the regular season and playoff games.

The XGBoost has the drop in ROC, compared to ROC in regular season. At that time, the generalization of the trained XGBoost has a little problem and need further exploration in the future.  

The performance of the trained decision tree is acceptable, and higher than the baseline logistic regression and even the XGboost model. 

**In conclusion**, we will choose the decision tree as our best model for two reasons. First, the generalization of the decision tree still maintain good performance in both regular and playoffs game (ROC = 0.75). Seconds, the training and inference time of the decision tree is faster than the XGBoost. 

The plot curve of ROC/AUC, Goal rate vs probability percentile, Cumulative proportion of goals vs probability percentile, and Reliability curve of the playoff games.
![](/Images/evaluation/playoff_evaluation.png)

---
