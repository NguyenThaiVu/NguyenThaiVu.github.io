---
layout: post
title:  "Milestone 2"
date:   2023-11-14 15:25:05 -0400
categories: Data science project
---

# Table of content:

---

## 1. Feature engineering 1

---
## 2. Baseline model

In the provided notebook, we implemented baseline models for predicting goal outcomes in hockey games. The notebook begins by importing necessary libraries, setting up directories for data, models, and figures, and initializing Comet experiments for experiment tracking. It then loads the dataset, preprocesses it by selecting desired features, and splits it into training and validation sets. Logistic regression models are trained separately for shot distance, shot angle, and a combination of both features. The notebook includes visualizations such as confusion matrices, ROC curves, goal rate, cumulative percentage of goal, and calibration curves for each model. Random predictions are also generated as a baseline. The notebook concludes with logging relevant metrics and saving the trained models, confusion matrices, and metrics on Comet for further analysis. The models and experiments are tagged appropriately, indicating their baseline nature and the features used in training.

### 2.1. Unveiling the Accuracy Paradox

![](/Images/baseline/hist.png)

The evaluation results indicate a high accuracy of 91% on the validation set, but upon closer inspection of the precision, recall, and f1-score for label 1 (goal), it becomes apparent that the model is unable to correctly predict instances of this class, yielding zeros in these metrics. The issue arises from the significant class imbalance in the dataset, where label 0 (non-goal) vastly outnumbers label 1. With 276,782 samples for label 0 and only 29,032 samples for label 1, the model might be biased towards predicting the majority class, achieving high accuracy due to the dominance of label 0 in the dataset. However, this high accuracy is misleading, as the model struggles to capture the minority class (label 1).


### 2.2. Evaluating Baselines

![](/Images/baseline/baseline-evaluation.png)

Baseline Model - Distance: [here](https://www.comet.com/ift6758-b09-project/ift6758-project-milestone2/fd6f683bf9324bc4aafe732516e9ed38)

Baseline Model - Angle: [here](https://www.comet.com/ift6758-b09-project/ift6758-project-milestone2/066eb71923294143887d23136514beb5)

Baseline Model - Distance + Angle: [here](https://www.comet.com/ift6758-b09-project/ift6758-project-milestone2/90775941400f48689503c7bacdc0ff09)

As we can see, the simplest baseline, random prediction, acts exactly the same as random models, having 50 percent area under the curve in the ROC plot and steady in goal rate and calibration plots. On the other hand, we have noticed slight improvement from the angle model to the distance model and then from there to the model using a combination of them showing that as we increase the number of features, we encounter better performance in evaluation.


---
## 3. Feature engineering 2

### 3.1. Add features

We added the features from the list below

| Feature Name                | Explanation                                                              |
|-----------------------------|--------------------------------------------------------------------------|
| game_second                 | Time (in seconds) of each event relative to the start of the entire game |
| last_event_type             | Type of the last event.                                                  |
| coor_x_last_event           | X-coordinate of the last event.                                          |
| coor_y_last_event           | Y-coordinate of the last event.                                          |
| time_last_event             | Time elapsed since the last event (seconds).                             |
| distance_last_event         | Distance from the last event.                                            |
| is_rebound                  | True if the last event was a rebound, otherwise False.                   |
| Change in shot angle        | Angle change in the shot, only applicable for rebounds.                  |
| Speed                       | Defined as distance from the previous event divided by time.             |
| power_play_time **(bonus)** | Time elapsed since the start of the power play (seconds).                |
| n_friend        **(bonus)** | Number of friendly non-goalie skaters on the ice.                        |
| n_oppose        **(bonus)** | Number of opposing non-goalie skaters on the ice.                        |


After adding the features, we processed the data:
- First we split the data into a training set and a test set (the function `split_train_test()` was used here), and the subsequent processing of the data will be different depending on the model chosen. 
- Then we removed features that were not relevant to the modeling (the function `remove_extra_features()` was used) and added a new feature `game_second`. 
- Finally we uploaded `train.csv`, `test_regular.csv`, `test_playoff.csv` to comet.ml.
You can download the dataset by the link [here](https://www.comet.com/ift6758-b09-project/artifacts/dataset/4.0.2)

### 3.2. Add json file

- For this particular match, we wrote a separate py file `get_game.py` to run it, and the purpose of this file is to upload this match as a dataframe to comet.ml, named according to requirements(`wpg_v_wsh_2017021065.csv`)
- You can check the link: [here](https://www.comet.com/ift6758-b09-project/ift6758-project-milestone2/c2985f085ae342bcbd8aa86aa9b3cb3c?experiment-tab=panels&showOutliers=true&smoothing=0&xAxis=step)


---
## 4. Advanced model


---
## 5. Best shot model

### 5.1. Data pre-processing

In this section, we will provide step-by-step our data pre-processing step, including: add feture, feature selection using correlation, feature selection using mutual information, and balance dataset.

#### i. Add feature

In this section, we will add the new feature called `attacking_zone_shot`. This feature indicates whether a shot took place within the opposing team's attacking zone. We choose to analyze this feature based on the observation that the shot in the attacking zone has high chance to become a goal.

After analyzing, we've discerned that 94.5% of shots occurred within the attacking zone, while the remaining 5% happened outside this zone.

#### ii. Feature selection - correlation

In this section, we will check the correlation value between pairs of features. If the correlation between two features is larger than the specific `threshold`, we will remove it. In this experiment, we set the threshold is 0.9.

After analyzing, we remove features `['period']`.

#### iii. Feature selection - mutual information

On this section, we will use mutual information to identify the relationship between features and the target variable. The intuition behind this experiment is that higher mutual information indicates stronger predictive power.

After calculating the mutual information between each feature and target variables. We got the result
![](/Images/best_shot_model/mutual_information.jpg)

According to the experiment, we will set the threshold below **0.01**. In other words, If the mutual information between each feature and target variable is smaller than 0.01, we will remove it. 

After analyzing, we choose feature:
`['x-coordinate', 'y-coordinate', 'shot_distance', 'angle', 'isEmptyNet', 'n_friend', 'n_oppose', 'last_event_type', 'is_rebound', 'attacking_zone_shot']`

#### iv. Balance dataset

Because the distribution of label in the training set is imbalanced. We apply the over-sample to add more samples to the minority class.

### 5.2. Machine learning model grid search

In this section, we run two type of models: decision tree and logistic regression with the preprocessed dataset. For the purpose of this experiment, we just run the simple grid search experiment and leave the complicated work later. The grid hyperparameter tuning on each model:
- Grid search decision tree:
```
param_grid_tree = {
    "tree__criterion": ['gini', 'entropy'],
    "tree__max_depth": [5, 10],
    "tree__min_samples_leaf": [5, 10]
}
```

- Grid search logistic regression:

```
param_grid_linear = {
    "linear_clf__penalty": ['l1', 'l2'],
    "linear_clf__C": [0.1, 0.01]
}
```
We create four figures (ROC/AUC curve, goal rate vs probability percentile, cumulative proportion of goals vs probability percentile, and the reliability curve) to both ML model: decision tree and knn

#### i. The ROC/AUC curve

![](/Images/best_shot_model/roc.jpg)


#### ii. The goal rate vs probability percentile

![](/Images/best_shot_model/goal_rate.jpg)


#### iii. The cumulative proportion of goals vs probability percentile
![](/Images/best_shot_model/goal_cumulative_proportion.jpg)


#### iii. The reliability curve

![](/Images/best_shot_model/calibration.jpg)


In addition, we also inspect the accuracy and f1 score on the validation set:

|                | Accuracy | F1 score |
| -------------- | -------- |--------  |
| Decision Tree  |  90.7%  | 0.738     |
| Logistic regression  |  90.3%  | 0.731     | 

In summary, the decision tree is the best model we can build.

### 5.3. Log the models to the experiments on comet.ml

We have add the trained decision tree into our Model Registry. 

You can download it with those links:
- Decision tree model: [here](https://www.comet.com/api/registry/model/item/download?modelItemId=ezJQ3TR5dmuvrVucdWsbKtYaR)

We also add the evaluation metrics to the Comet experiment. We add 3 metrics, including: Accuracy, ROC, and confusion matrix. 
You can access each individual experiment with the tag `best_shot_model` for more details. 

In case you want to check the metrics each experiment in a tabular format, you can check via this link: 
[](https://www.comet.com/ift6758-b09-project/ift6758-project-milestone2/view/new/experiments)



